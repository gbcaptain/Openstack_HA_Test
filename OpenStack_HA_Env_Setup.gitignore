# Outline
- Env Deployment prepare;
    - 6 VM (4c + 16G Mem +100G disk + 4 NIC)
    - CentOS7.1 x64
    - YUM repos (Openstack + CentOS7.2 + EPEL)

- install MariaDB Galera Cluster;
[root@controller1 ~]# vim /etc/hosts

[root@controller1 ~]# vim /etc/yum.repos.d/mariadb.repo
[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.3.7/centos7-amd64/
enabled = 1
gpgcheck = 0

[root@controller1 ~]# scp /etc/yum.repos.d/mariadb.repo controller2:/etc/yum.repos.d/mariadb.repo 
[root@controller1 ~]# scp /etc/yum.repos.d/mariadb.repo controller3:/etc/yum.repos.d/mariadb.repo

[root@controller1 ~]# yum clean all
[root@controller1 ~]# yum makecache
...
...

[root@controller1 ~]# yum install -y MariaDB-server MariaDB-client galera xinetd rsync
...
...

[root@controller1 ~]# systemctl start mariadb.service
[root@controller1 ~]# mysql_secure_installation
[root@controller1 ~]# vim /etc/my.cnf.d/server.cnf
[galera]
wsrep_provider = /usr/lib64/galera/libgalera_smm.so
wsrep_cluster_address = "gcomm://172.31.20.140,172.31.109.87,172.31.102.187"
wsrep_node_name = controller1
wsrep_node_address = 172.31.20.140
wsrep_on = ON
binlog_format = ROW
max_connections = 10000
default_storage_engine = InnoDB
innodb_autoinc_lock_mode = 2
bind-address = 172.31.20.140
wsrep_slave_threads = 1
innodb_flush_log_at_trx_commit = 0
innodb_buffer_pool_size = 122M
wsrep_sst_method =rsync

[root@controller1 ~]# vim /usr/lib/systemd/system/mariadb.service
LimitNOFILE = 10000
LimitNPROC = 10000

[root@controller1 ~]# systemctl stop mariadb.service
[root@controller1 ~]# systemctl daemon-reload
...
...

[root@controller1 ~]# /usr/sbin/mysqld --wsrep-new-cluster --user=root &

[root@controller2 ~]# systemctl start mariadb.service
...

- install RabbitMQ Cluster;
[root@controller1 ~]# yum install -y erlang
[root@controller1 ~]# yum install -y rabbitmq-server
[root@controller1 ~]# systemctl enable rabbitmq-server.service
[root@controller1 ~]# systemctl restart rabbitmq-server.service
[root@controller1 ~]# systemctl list-unit-files | grep rabbitmq-server.service
[root@controller1 ~]# rabbitmqctl add_user openstack password
[root@controller1 ~]# /usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management mochiweb webmachine rabbitmq_web_dispatch amqp_client rabbitmq_management_agent
[root@controller1 ~]# systemctl restart rabbitmq-server.service
[root@controller1 ~]# systemctl status rabbitmq-server.service
login via web browser:
http://[controller1_public_ip:15672]
...
...

[root@controller1 ~]# rabbitmqctl add_user mqadmin mqadmin
[root@controller1 ~]# rabbitmqctl set_user_tags mqadmin administrator
[root@controller1 ~]# rabbitmqctl set_permissions -p / mqadmin ".*" ".*" ".*"
[root@controller1 ~]# rabbitmqctl status

[root@controller1 ~]# scp /var/lib/rabbitmq/.erlang.cookie controller2:/var/lib/rabbitmq/.erlang.cookie
[root@controller1 ~]# scp /var/lib/rabbitmq/.erlang.cookie controller3:/var/lib/rabbitmq/.erlang.cookie

[root@controller2 ~]# rabbitmqctl stop_app
[root@controller2 ~]# rabbitmqctl join_cluster --ram rabbit@controller1
[root@controller2 ~]# rabbitmqctl start_app
...

[root@controller1 ~]# rabbitmqctl cluster_status

- install Pacemaker;
[root@controller1 ~]# cd /etc/yum.repos.d
[root@controller1 ~]# vim ha-clustering.repo
[network_ha-clustering_Stable]
name = Stable High Availability/Clustering packages (CentOS_CentOS-7)
type = rpm-md 
baseurl = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/
gpgcheck = 1
gpgkey = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/repodata/repomd.xml.key
enabled = 1

[root@controller1 ~]# scp ha-clustering.repo controller2:/etc/yum.repos.d/ha-clustering.repo
[root@controller1 ~]# scp ha-clustering.repo controller3:/etc/yum.repos.d/ha-clustering.repo

[root@controller1 ~]# cd /etc/yum.repos.d/
[root@controller1 ~]# yum install -y lvm2 cifs-utils quota psmisc
[root@controller1 ~]# yum install -y pcs pacemaker corosync fence-agents-all resource-agents crmsh lvm2 cifs cifs-utils quota psmisc
[root@controller1 ~]# systemctl enable pcsd
[root@controller1 ~]# systemctl enable corosync
[root@controller1 ~]# systemctl start pcsd
[root@controller1 ~]# systemctl status pcsd
[root@controller1 ~]# passwd hacluster
...
...
[root@controller1 ~]# vim /etc/corosync/corosync.conf
totem {
    version: 2
    secauth: off
    cluster_name: openstack-cluster
    transport: udpu 
}

nodelist {
    node {
        ring0_addr: controller1
        nodeid: 1
    }
    node {
        ring0_addr: controller2
        nodeid: 2
    }
    node {
        ring0_addr: controller3
        nodeid: 3
    }
}

quorum {
    provider: corosync_votequorum
}

logging {
    to_logfile: yes
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
}

[root@controller1 ~]# scp /etc/corosync/corosync.conf controller2:/etc/corosync/corosync.conf
[root@controller1 ~]# scp /etc/corosync/corosync.conf controller3:/etc/corosync/corosync.conf

[root@controller1 ~]# ssh-keygen -t rsa 
...
...
[root@controller1 ~]# scp /root/.ssh/id_rsa.pub 9.110.187.121:~/.ssh/authorized_keys
[root@controller1 ~]# scp /root/.ssh/id_rsa.pub 9.110.187.122:~/.ssh/authorized_keys

[root@controller1 ~]# pcs cluster auth controller1 controller2 controller3 -u hacluster -p password --force
[root@controller1 ~]# pcs cluster setup --force  --name openstack-cluster controller1 controller2 controller3
[root@controller1 ~]# pcs cluster enable --all
[root@controller1 ~]# pcs cluster start --all
[root@controller1 ~]# pcs cluster status
[root@controller1 ~]# ps aux | grep pacemaker
[root@controller1 ~]# corosync-cfgtool -s
[root@controller1 ~]# corosync-cmapctl | grep members
[root@controller1 ~]# pcs status corosync
[root@controller1 ~]# crm-verify -L -V
[root@controller1 ~]# pcs property set stonith-enabled=false
[root@controller1 ~]# pcs property set no-quorum-policy=ignore
[root@controller1 ~]# crm-verify -L -V


- install HAProxy;
- install & config Keystone;
- install & config glance;
- install & config nova;
- install & config cinder;
- install & config neutron;
- install Dashboard;
- add Serv & Res to Pacemaker;
- Compute node deployment.

# Virt Env list
3 x Controller Nodes (8G, 300GB - )
    10.1.1.120 controller1 controller1.test.com                                 9.110.187.120
    10.1.1.121 controller2 controller2.test.com                                 9.110.187.121
    10.1.1.122 controller3 controller3.test.com                                 9.110.187.122
2 x Compute Nodes (16G, 300GB - )
    10.1.1.133 compute1 compute1.test.com
    10.1.1.134 compute2 compute2.test.com (glance-backend backend.test.com)
1 x Cinder Node (8G 400GB -)
    10.1.1.135 cinder1 cinder1.test.com

9.110.187.128 demo.openstack.com

# Network Config
public : 9.110.187.0/24
admin : 192.168.56.0/24
private：192.168.57.0/24
mapping network port：eno67109408

