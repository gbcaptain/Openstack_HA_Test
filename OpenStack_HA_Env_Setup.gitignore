# Outline
- Env Deployment prepare;
    - 6 VM (4c + 16G Mem +100G disk + 4 NIC)
    - CentOS7.1 x64
    - YUM repos (Openstack + CentOS7.2 + EPEL)

- install MariaDB Galera Cluster;
[root@controller1 ~]# vim /etc/hosts

[root@controller1 ~]# vim /etc/yum.repos.d/mariadb.repo
[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.3.7/centos7-amd64/
enabled = 1
gpgcheck = 0

[root@controller1 ~]# scp /etc/yum.repos.d/mariadb.repo controller2:/etc/yum.repos.d/mariadb.repo 
[root@controller1 ~]# scp /etc/yum.repos.d/mariadb.repo controller3:/etc/yum.repos.d/mariadb.repo

[root@controller1 ~]# yum clean all
[root@controller1 ~]# yum makecache
...
...

[root@controller1 ~]# yum install -y MariaDB-server MariaDB-client galera xinetd rsync
...
...

[root@controller1 ~]# systemctl start mariadb.service
[root@controller1 ~]# mysql_secure_installation
[root@controller1 ~]# vim /etc/my.cnf.d/server.cnf
[galera]
wsrep_provider = /usr/lib64/galera/libgalera_smm.so
wsrep_cluster_address = "gcomm://192.168.56.120,192.168.56.121,192.168.56.122"
wsrep_node_name = controller1
wsrep_node_address = 192.168.56.120
wsrep_on = ON
binlog_format = ROW
max_connections = 10000
default_storage_engine = InnoDB
innodb_autoinc_lock_mode = 2
bind-address = 192.168.56.120
wsrep_slave_threads = 1
innodb_flush_log_at_trx_commit = 0
innodb_buffer_pool_size = 122M
wsrep_sst_method =rsync

[root@controller1 ~]# vim /usr/lib/systemd/system/mariadb.service
LimitNOFILE = 10000
LimitNPROC = 10000

[root@controller1 ~]# systemctl stop mariadb.service
[root@controller1 ~]# systemctl daemon-reload
...
...

[root@controller1 ~]# /usr/sbin/mysqld --wsrep-new-cluster --user=root &

[root@controller2 ~]# systemctl start mariadb.service
...
[ISSUE] - mariadb failed to start, 'rsync' not found in PATH 
[FIX]: change SELinux from "enforcing" to "permissive" in /etc/sysconfig/SELinux

[root@controller2 ~]# systemctl status mariadb.service
...
[root@controller2 ~]# mysql -uroot -p
 MariaDB [(none)]> show status like 'wsrep%';
 MariaDB [(none)]> show databases;
 MariaDB [(none)]> CREATE DATABASE test;



- install RabbitMQ Cluster;
[root@controller1 ~]# yum install -y epel-release
[root@controller1 ~]# yum install -y erlang
[root@controller1 ~]# yum install -y rabbitmq-server
[root@controller1 ~]# systemctl enable rabbitmq-server.service
[root@controller1 ~]# systemctl restart rabbitmq-server.service
[root@controller1 ~]# systemctl list-unit-files | grep rabbitmq-server.service
[root@controller1 ~]# rabbitmqctl add_user openstack password
[root@controller1 ~]# /usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management mochiweb webmachine rabbitmq_web_dispatch amqp_client rabbitmq_management_agent
[root@controller1 ~]# systemctl restart rabbitmq-server.service
[root@controller1 ~]# systemctl status rabbitmq-server.service
login via web browser:
http://[controller1_public_ip:15672]
...
...

[root@controller1 ~]# rabbitmqctl add_user mqadmin mqadmin
[root@controller1 ~]# rabbitmqctl set_user_tags mqadmin administrator
[root@controller1 ~]# rabbitmqctl set_permissions -p / mqadmin ".*" ".*" ".*"
[root@controller1 ~]# rabbitmqctl status

[root@controller1 ~]# scp /var/lib/rabbitmq/.erlang.cookie controller2:/var/lib/rabbitmq/.erlang.cookie
[root@controller1 ~]# scp /var/lib/rabbitmq/.erlang.cookie controller3:/var/lib/rabbitmq/.erlang.cookie

[root@controller2 ~]# systemctl restart rabbitmq-server
[root@controller2 ~]# rabbitmqctl stop_app
[root@controller2 ~]# rabbitmqctl start_app
[root@controller2 ~]# rabbitmqctl join_cluster --ram rabbit@controller1
[root@controller2 ~]# rabbitmqctl start_app
...

[root@controller1 ~]# rabbitmqctl cluster_status

- install Pacemaker;
[root@controller1 ~]# cd /etc/yum.repos.d
[root@controller1 ~]# vim ha-clustering.repo
[network_ha-clustering_Stable]
name = Stable High Availability/Clustering packages (CentOS_CentOS-7)
type = rpm-md 
baseurl = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/
gpgcheck = 1
gpgkey = http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/repodata/repomd.xml.key
enabled = 1

[root@controller1 ~]# scp ha-clustering.repo controller2:/etc/yum.repos.d/ha-clustering.repo
[root@controller1 ~]# scp ha-clustering.repo controller3:/etc/yum.repos.d/ha-clustering.repo

[root@controller1 ~]# cd /etc/yum.repos.d/
[root@controller1 ~]# yum install -y lvm2 cifs-utils quota psmisc
[root@controller1 ~]# yum install -y pcs pacemaker corosync fence-agents-all resource-agents crmsh lvm2 cifs cifs-utils quota psmisc
[root@controller1 ~]# systemctl enable pcsd
[root@controller1 ~]# systemctl enable corosync
[root@controller1 ~]# systemctl start pcsd
[root@controller1 ~]# systemctl status pcsd
[root@controller1 ~]# passwd hacluster
...
...
[root@controller1 ~]# vim /etc/corosync/corosync.conf
totem {
    version: 2
    secauth: off
    cluster_name: openstack-cluster
    transport: udpu 
}

nodelist {
    node {
        ring0_addr: controller1
        nodeid: 1
    }
    node {
        ring0_addr: controller2
        nodeid: 2
    }
    node {
        ring0_addr: controller3
        nodeid: 3
    }
}

quorum {
    provider: corosync_votequorum
}

logging {
    to_logfile: yes
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
}

[root@controller1 ~]# scp /etc/corosync/corosync.conf controller2:/etc/corosync/corosync.conf
[root@controller1 ~]# scp /etc/corosync/corosync.conf controller3:/etc/corosync/corosync.conf

[root@controller1 ~]# ssh-keygen -t rsa 
...
...
[root@controller1 ~]# scp /root/.ssh/id_rsa.pub 9.110.187.121:~/.ssh/authorized_keys
[root@controller1 ~]# scp /root/.ssh/id_rsa.pub 9.110.187.122:~/.ssh/authorized_keys

[root@controller1 ~]# pcs cluster auth controller1 controller2 controller3 -u hacluster -p password --force
[root@controller1 ~]# pcs cluster setup --force  --name openstack-cluster controller1 controller2 controller3
[root@controller1 ~]# pcs cluster enable --all
[root@controller1 ~]# pcs cluster start --all
[root@controller1 ~]# pcs cluster status
[root@controller1 ~]# ps aux | grep pacemaker
[root@controller1 ~]# corosync-cfgtool -s
[root@controller1 ~]# corosync-cmapctl | grep members
[root@controller1 ~]# pcs status corosync
[root@controller1 ~]# crm_verify -L -V
[root@controller1 ~]# pcs property set stonith-enabled=false
[root@controller1 ~]# pcs property set no-quorum-policy=ignore
[root@controller1 ~]# crm_verify -L -V

- install HAProxy;

[root@controller1 ~]# yum -y install haproxy
[root@controller1 ~]# systemctl enable haproxy.service
[root@controller1 ~]# cd /etc/rsyslog.d/
[root@controller1 ~]# vim haproxy.conf
$ModLoad imudp
$UDPServerRun 514
$template Haproxy,"%msg%n"
local0.=info -/var/log/haproxy.log;Haproxy
local0.notice -/var/log/haproxy-status.log;Haproxy
local0.* ~

[root@controller1 ~]# scp haproxy.conf controller2:/etc/rsyslog.d/haproxy.conf
[root@controller1 ~]# scp haproxy.conf controller3:/etc/rsyslog.d/haproxy.conf

[root@controller1 ~]# systemctl restart rsyslog.service
[root@controller1 ~]# systemctl status rsyslog.service
...
...
[root@controller1 ~]# cd /etc/haproxy/
[root@controller1 ~]# mv haproxy.cfg haproxy.cfg.orig
[root@controller1 ~]# vim haproxy.cfg 
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
 global
  log 127.0.0.1 local0
  log 127.0.0.1 local1 notice
  maxconn  4096
  chroot  /var/lib/haproxy
  daemon
  group  haproxy
  user  haproxy
#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will 
# use if not designated in their block
#---------------------------------------------------------------------
defaults
  log  global
  mode http
  maxconn  4000
  option tcplog
  option dontlognull
  option  redispatch
  retries  3
  contimeout 5000
  clitimeout 50000
  srvtimeout 50000

frontend stats-frontend
    bind *:8088
    mode http
    default_backend stats-back

backend stats-back 
    mode http
    blance source

#---------------------------------------------------------------------
# Listen webfarm
#---------------------------------------------------------------------
 listen dashboard_cluster
  bind 192.168.56.128:443
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 controller1:443 check inter 2000 rise 2 fall 5
  server controller2 controller2:443 check inter 2000 rise 2 fall 5
  server controller3 controller3:443 check inter 2000 rise 2 fall 5

listen Galera-Cluster

    stats auth admin:password

listen Galera-Cluster
    bind 192.168.56.128:3306
    balance leastconn
    mode tcp
    option httpchk
    server controller1  controller1:3306 check port 9200 inter 2000 rise 2 fall 3
    server controller2  controller2:3306 check port 9200 inter 2000 rise 2 fall 3
    server controller3  controller3:3306 check port 9200 inter 2000 rise 2 fall 3

listen keystone_admin_cluster
    bind 192.168.56.128:35357
    balance source
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1  controller1:35358 check inter 2000 rise 2 fall 5
    server controller2  controller2:35358 check inter 2000 rise 2 fall 5
    server controller3  controller3:35358 check inter 2000 rise 2 fall 5

listen keystone_public_internal_cluster
    bind 192.168.56.128:5000
    balance  source
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1 controller1:5000 check inter 2000 rise 2 fall 5
    server controller2 controller2:5000 check inter 2000 rise 2 fall 5
    server controller3 controller3:5000 check inter 2000 rise 2 fall 5

listen dashboard_cluster
  bind 192.168.56.128:80
  balance  source
  option  tcpka
  option  tcplog
  option  httpchk
  server controller1 controller1:8080 check inter 2000 rise 2 fall 5
  server controller2 controller2:8080 check inter 2000 rise 2 fall 5
  server controller3 controller3:8080 check inter 2000 rise 2 fall 5

listen keystone_admin_cluster
    bind 192.168.56.128:35357
    balance source
    option  tcpka
    option  httpchk
    option  tcplog
    server controller1  controller1:35358 check inter 2000 rise 2 fall 5
    server controller2  controller2:35358 check inter 2000 rise 2 fall 5
    server controller3  controller3:35358 check inter 2000 rise 2 fall 5

listen glance_api_cluster
  bind 192.168.56.128:9292
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 controller1:9292 check inter 2000 rise 2 fall 5
  server controller2 controller2:9292 check inter 2000 rise 2 fall 5
  server controller3 controller3:9292 check inter 2000 rise 2 fall 5

listen cinder_api_cluster
  bind 192.168.56.128:8776
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 controller1:8778 check inter 2000 rise 2 fall 5
  server controller2 controller2:8778 check inter 2000 rise 2 fall 5
  server controller3 controller3:8778 check inter 2000 rise 2 fall 5

listen nova_vncproxy_cluster
  bind 192.168.56.128:6080
  balance  source
  option  tcpka
  option  tcplog
  server controller1 controller1:6081 check inter 2000 rise 2 fall 5
  server controller2 controller2:6081 check inter 2000 rise 2 fall 5
  server controller3 controller3:6081 check inter 2000 rise 2 fall 5

listen neutron_api_cluster
  bind 192.168.56.128:9696
  balance  source
  option  tcpka
  option  httpchk
  option  tcplog
  server controller1 controller1:9697 check inter 2000 rise 2 fall 5
  server controller2 controller2:9697 check inter 2000 rise 2 fall 5
  server controller3 controller3:9697 check inter 2000 rise 2 fall 5

[root@controller1 ~]# scp haproxy.cfg controller2:/etc/haproxy/haproxy.cfg
[root@controller1 ~]# scp haproxy.cfg controller3:/etc/haproxy/haproxy.cfg

[root@controller1 ~]# systemctl start haproxy.service
[root@controller1 ~]# systemctl status haproxy.service
[root@controller1 ~]# mysql -uroot -p
MariaDB [(none)]> grant process on *.* to 'clustercheckuser'@'localhost' identified by 'clustercheckpassword'
MariaDB [(none)]> flush privileges;
MariaDB [(none)]> exit
[root@controller1 ~]# vim /etc/sysconfig/clustercheck
MYSQL_USERNAME=clustercheckuser
MYSQL_PASSWORD=clustercheckpassword!
MYSQL_HOST=localhost
MYSQL_PORT=3306
[root@controller1 ~]# scp /etc/sysconfig/clustercheck controller2:/etc/sysconfig/clustercheck
[root@controller1 ~]# scp /etc/sysconfig/clustercheck controller3:/etc/sysconfig/clustercheck
[root@controller1 ~]# vim /usr/bin/clustercheck
#!/bin/bash
#
# Script to make a proxy (ie HAProxy) capable of monitoring Percona XtraDB Cluster nodes properly
#
# Author: Olaf van Zandwijk <olaf.vanzandwijk@nedap.com>
# Author: Raghavendra Prabhu <raghavendra.prabhu@percona.com>
#
# Documentation and download: https://github.com/olafz/percona-clustercheck
#
# Based on the original script from Unai Rodriguez
#

if [[ $1 == '-h' || $1 == '--help' ]];then
    echo "Usage: $0 <user> <pass> <available_when_donor=0|1> <log_file> <available_when_readonly=0|1> <defaults_extra_file>"
    exit
fi

# if the disabled file is present, return 503. This allows
# admins to manually remove a node from a cluster easily.
if [ -e "/var/tmp/clustercheck.disabled" ]; then
    # Shell return-code is 1
    echo -en "HTTP/1.1 503 Service Unavailable\r\n"
    echo -en "Content-Type: text/plain\r\n"
    echo -en "Connection: close\r\n"
    echo -en "Content-Length: 51\r\n"
    echo -en "\r\n"
    echo -en "Percona XtraDB Cluster Node is manually disabled.\r\n"
    sleep 0.1
    exit 1
fi

set -e

if [ -f /etc/sysconfig/clustercheck ]; then
        . /etc/sysconfig/clustercheck
fi

MYSQL_USERNAME="${MYSQL_USERNAME:=-clustercheckuser}"
MYSQL_PASSWORD="${MYSQL_PASSWORD-clustercheckpassword!}"
AVAILABLE_WHEN_DONOR=${AVAILABLE_WHEN_DONOR:-0}
ERR_FILE="${ERR_FILE:-/dev/null}"
AVAILABLE_WHEN_READONLY=${AVAILABLE_WHEN_READONLY:-1}
DEFAULTS_EXTRA_FILE=${DEFAULTS_EXTRA_FILE:-/etc/my.cnf}

#Timeout exists for instances where mysqld may be hung
TIMEOUT=10

EXTRA_ARGS=""
if [[ -n "$MYSQL_USERNAME" ]]; then
    EXTRA_ARGS="$EXTRA_ARGS --user=${MYSQL_USERNAME}"
fi
if [[ -n "$MYSQL_PASSWORD" ]]; then
    EXTRA_ARGS="$EXTRA_ARGS --password=${MYSQL_PASSWORD}"
fi
if [[ -r $DEFAULTS_EXTRA_FILE ]];then
    MYSQL_CMDLINE="mysql --defaults-extra-file=$DEFAULTS_EXTRA_FILE -nNE --connect-timeout=$TIMEOUT \
                    ${EXTRA_ARGS}"
else
    MYSQL_CMDLINE="mysql -nNE --connect-timeout=$TIMEOUT ${EXTRA_ARGS}"
fi
#
# Perform the query to check the wsrep_local_state
#
WSREP_STATUS=$($MYSQL_CMDLINE -e "SHOW STATUS LIKE 'wsrep_local_state';" \
    2>${ERR_FILE} | tail -1 2>>${ERR_FILE})

if [[ "${WSREP_STATUS}" == "4" ]] || [[ "${WSREP_STATUS}" == "2" && ${AVAILABLE_WHEN_DONOR} == 1 ]]
then
    # Check only when set to 0 to avoid latency in response.
    if [[ $AVAILABLE_WHEN_READONLY -eq 0 ]];then
        READ_ONLY=$($MYSQL_CMDLINE -e "SHOW GLOBAL VARIABLES LIKE 'read_only';" \
                    2>${ERR_FILE} | tail -1 2>>${ERR_FILE})

        if [[ "${READ_ONLY}" == "ON" ]];then
            # Percona XtraDB Cluster node local state is 'Synced', but it is in
            # read-only mode. The variable AVAILABLE_WHEN_READONLY is set to 0.
            # => return HTTP 503
            # Shell return-code is 1
            echo -en "HTTP/1.1 503 Service Unavailable\r\n"
            echo -en "Content-Type: text/plain\r\n"
            echo -en "Connection: close\r\n"
            echo -en "Content-Length: 43\r\n"
            echo -en "\r\n"
            echo -en "Percona XtraDB Cluster Node is read-only.\r\n"
            sleep 0.1
            exit 1
        fi
    fi
    # Percona XtraDB Cluster node local state is 'Synced' => return HTTP 200
    # Shell return-code is 0
    echo -en "HTTP/1.1 200 OK\r\n"
    echo -en "Content-Type: text/plain\r\n"
    echo -en "Connection: close\r\n"
    echo -en "Content-Length: 40\r\n"
    echo -en "\r\n"
    echo -en "Percona XtraDB Cluster Node is synced.\r\n"
    sleep 0.1
    exit 0
else
    # Percona XtraDB Cluster node local state is not 'Synced' => return HTTP 503
    # Shell return-code is 1
    echo -en "HTTP/1.1 503 Service Unavailable\r\n"
    echo -en "Content-Type: text/plain\r\n"
    echo -en "Connection: close\r\n"
    echo -en "Content-Length: 44\r\n"
    echo -en "\r\n"
    echo -en "Percona XtraDB Cluster Node is not synced.\r\n"
    sleep 0.1
    exit 1
fi

[root@controller1 ~]# scp /usr/bin/clustercheck controller2:/usr/bin/clustercheck
[root@controller1 ~]# scp /usr/bin/clustercheck controller3:/usr/bin/clustercheck

[root@controller1 ~]# chmod +x /usr/bin/clustercheck
[root@controller1 ~]# clustercheck

[root@controller1 ~]# yum -y install xinetd
[root@controller1 ~]# vim /etc/xinetd.d/mysqlchk
service mysqlchk
{
# # this is a config for xinetd, place it in /etc/xinetd.d/
    disable = no
    flags = REUSE
    socket_type = stream
    port = 9200
    wait = no
    user = nobody
    server = /usr/bin/clustercheck
    log_on_failure += USERID
    only_from = 0.0.0.0/0
    # recommended to put the IPs that need 
    # to connect exclusively (security purposes)
    per_source = UNLIMITED
}
[root@controller1 ~]# scp /etc/xinetd.d/mysqlchk controller2:/etc/xinetd.d/mysqlchk
[root@controller1 ~]# scp /etc/xinetd.d/mysqlchk controller3:/etc/xinetd.d/mysqlchk

[root@controller1 ~]# vim /etc/services
+
mysqlchk    9200/tcp    # mysqlchk
+
...
...
[root@controller1 ~]# systemctl restart xinetd.service
[root@controller1 ~]# systemctl status xinetd.service

[root@controller1 haproxy]# echo 'net.ipv4.ip_nonlocal_bind = 1' >>/etc/sysctl.conf
[root@controller1 haproxy]# echo 'net.ipv4.ip_forward = 1' >>/etc/sysctl.conf
[root@controller1 haproxy]# sysctl -p
...
...

[root@controller1 ~]# systemctl start haproxy.service
[root@controller1 ~]# systemctl status haproxy.service


- install & config Keystone;
[root@controller1 ~]# mysql -uroot -p
MariaDB [(none)]> CREATE DATABASE keystone;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'passord';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'passord';
MariaDB [(none)]> exit
[root@controller1 ~]# yum -y install openstack-keystone httpd mod_wsgi python-openstackclient memcached python-memcached openstack-utils
[root@controller1 ~]# systemctl enable memcached.service
[root@controller1 ~]# systemctl start memcached.service
[root@controller1 ~]# systemctl status memcached.service

[root@controller1 ~]# openssl rand -hex 10
[root@controller1 ~]# ADMIN_TOKEN=[generated_token]
[root@controller1 ~]# mv /etc/keystone/keystone.conf /etc/keystone/keystone.conf.bak 
[root@controller1 ~]# vim /etc/keystone/keystone.conf
[default]
verbose =  True
admin_token = [generated_token]
bind_host = controller1
public_bind_host = 192.168.56.120
admin_bind_host = 192.168.56.120

[database]
connection = mysql://keystone:password@demon.openstack.com/keystone

[memcache]
servers = controller1:11211,controller2:11211,controller3:11211

[token]
caching = True
provider = keystone.token.providers.uuid.Provider 
driver = keystone.token.persistence.backends.memcache.Token
token = keystone.auth.plugins.token.Token

[revoke]
driver = keystone.contrib.revoke.backends.sql.Revoke

[catalog]
driver = keystone.identity.backends.sql.Identity

[root@controller1 ~]# chown root:keystone /etc/keystone/keystone.conf
[root@controller1 ~]# chmod 640 /etc/keystone/keystone.conf
...+
...+

[root@controller1 ~]# vim /etc/httpd/conf/httpd.conf
+
ServerName controller1

Listen 8080
+
...
...

[root@controller1 ~]# vim /etc/httpd/conf.d/wsgi-keystone.conf
Listen 5002
Listen 35358

<VirtualHost *:5002>
    WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-public
    WSGIScriptAlias / /usr/bin/keystone-wsgi-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization ON
    ErrorLogFormat "%{cu}t %M"
    ErrorLog /var/log/httpd/keystone-error.log
    CustomLog /var/log/httpd/keystone-access.log combined

    <Directory /usr/bin>
        Require all granted
    </Directory>
</VirtualHost>

<VirtualHost *:35358>
    WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-admin
    WSGIScriptAlias / /usr/bin/keystone-wsgi-admin
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization ON
    ErrorLogFormat "%{cu}t %M"
    ErrorLog /var/log/httpd/keystone-error.log
    CustomLog /var/log/httpd/keystone-access.log combined

    <Directory /usr/bin>
        Require all granted
    </Directory>
</VirtualHost>

[root@controller1 ~]# scp /etc/httpd/conf.d/wsgi-keystone.conf controller2:/etc/httpd/conf.d/wsgi-keystone.conf
[root@controller1 ~]# scp /etc/httpd/conf.d/wsgi-keystone.conf controller3:/etc/httpd/conf.d/wsgi-keystone.conf

[root@controller1 ~]# su - /bin/sh -c "keystone-manage db_sync" keystone

[root@controller1 ~]# cd /var/log/keystone/
[root@controller1 ~]# vim keystone.log

[root@controller1 ~]# mysql -uroot -p
MariaDB [(none)]> show databases;
MariaDB [(none)]> exit

[root@controller1 ~]# vim /etc/keystone/keystone.conf

[root@controller1 keystone]# crm
crm(live)# configure
crm(live)configure# primitive vip ocf:hearbeat:IPaddr2 params ip=9.110.187.128 cidr_netmask=24 nic=ens160 op start interval=0s timeout=20s op stop interval=0s timeout=20s op monitor interval=30s meta priority=100
crm(live)configure# show
crm(live)configure# exit

[root@controller1 keystone]# crm
crm(live)# status
crm(live)# exit

[root@controller1 keystone]# ping demo.openstack.com
[root@controller1 ~]# su - /bin/sh -c "keystone-manage db_sync" keystone

[root@controller1 ~]# mysql -uroot -p
MariaDB [(none)]> show databases;
MariaDB [(none)]> use keystone;
MariaDB [(none)]> show tables;
MariaDB [(none)]> exit

[root@controller1 ~]# systemctl enable httpd.service
[root@controller1 ~]# systemctl start httpd.service
[root@controller1 ~]# systemctl list-unit-files | grep httpd.service

[root@controller1 keystone]# keystone-manage bootstrap \
            --bootstrap-password password \
            --bootstrap-username admin \
            --bootstrap-project-name admin \
            --bootstrap-role-name admin \
            --bootstrap-service-name keystone \
            --bootstrap-region-id RegionOne \
            --bootstrap-admin-url http://demo.openstack.com:35357 \
            --bootstrap-public-url http://demo.openstack.com:5000 \
            --bootstrap-internal-url http://demo.openstack:5000
[root@controller1 keystone]# openstack project list --os-username admin --os-project-name admin --os-user-domain-id default --os-project-domain-id default --os-identity-api-version 3 --os-auth-url http://demo.openstack.com:5000 --os-password password
[root@controller1 keystone]# vim /root/admin-openrc
export OS_USER_DOMAIN_ID=default
export OS_PROJECT_DOMAIN_ID=default
export OS_USERNAME=admin
export OS_PROJECT_NAME=admin
export OS_PASSWORD=password
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_URL=http://demo.openstack.com:5000

[root@controller1 keystone]# source /root/admin-openrc
[root@controller1 keystone]# openstack endpoint create --region RegionOne identity public http://demo.openstack.com:5000/v3
[root@controller1 keystone]# openstack endpoint create --region RegionOne identity admin http://demo.openstack.com:35357/v3
[root@controller1 keystone]# openstack endpoint create --region RegionOne identity internal http://demo.openstack.com:5000/v3
[root@controller1 keystone]# openstack endpoint create --domain default --description "Service Project" service
[root@controller1 keystone]# openstack endpoint create --domain default --description "Demo Project" demo
[root@controller1 keystone]# openstack user create --domain default demo --password password
[root@controller1 keystone]# openstack role create user
[root@controller1 keystone]# openstack role add --project demo --user demo user
[root@controller1 keystone]# unset OS_TOKEN OS_URL 
[root@controller1 keystone]# openstack --os-auth-url http://9.110.187.128:35357/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name admin --os-username admin token issue --os-password password
[root@controller1 keystone]# openstack --os-auth-url http://9.110.187.128:5000/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name demo --os-username demo token issue --os-password password
[root@controller1 keystone]# vim /root/demo-openrc
export OS_USER_DOMAIN_ID=default
export OS_PROJECT_DOMAIN_ID=default
export OS_USERNAME=demo
export OS_PROJECT_NAME=demo
export OS_PASSWORD=password
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_URL=http://demo.openstack.com:5000

[root@controller1 keystone]# cd ~
[root@controller1 ~]# scp admin-openrc controller2:/root/admin-openrc
[root@controller1 ~]# scp admin-openrc controller3:/root/admin-openrc
[root@controller1 ~]# scp demo-openrc controller2:/root/demo-openrc
[root@controller1 ~]# scp demo-openrc controller3:/root/demo-openrc

- install & config glance;
[root@controller1 ~]# mysql -uroot -p
MariaDB [(none)]> CREATE DATABASE glance;
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'password';
MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'password';
MariaDB [(none)]> exit
[root@controller1 ~]# source /root/admin-openrc
[root@controller1 ~]# openstack user create --domain default glance --password password
[root@controller1 ~]# openstack role add --project service --user glance admin
[root@controller1 ~]# openstack service create --name glance --description "Openstack Image service" image
[root@controller1 ~]# openstack endpoint create --region RegionOne image public http://demo.openstack.com:9292 
[root@controller1 ~]# openstack endpoint create --region RegionOne image internal http://demo.openstack.com:9292 
[root@controller1 ~]# openstack endpoint create --region RegionOne image admin http://demo.openstack.com:9292 

[root@controller1 ~]# yum install openstack-glance python-glance python-glanceclient -y
...
...
[root@controller1 rsyslog.d]# >/etc/glance/glance-api.conf
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT show_image_direct_url false
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT debug True
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT verbose True
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT bind_host controller1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT bind_port 9393
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT registry_host controller1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT registry_port 9191
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT auth_region RegionOne
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT registry_client_protocol http
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT show_image_direct_url false
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT worker 4
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT imag_cache_dir /var/lib/glance/image-cache
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT rpc_backend rabbit
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf DEFAULT delayed_delete false
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_host 9.110.187.128
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_port 5672
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_userid openstack
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_password password
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_use_ssl false
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_ha_queues True
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_retry_interval 1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_retry_backoff 2
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit rabbit_max_retries 0
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf oslo_messaging_rabbit amqp_durable_queues false
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:password@demo.openstack.com/glance
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf database idle_timeout 3600
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf database max_pool_size 30
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf database max_retries -1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf database max_overflow 60
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://demo.openstack.com:5000
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://demo.openstack.com:35357
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller1:11211,controller2:11211,controller3:11211
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password password
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf keystone_authtoken token_cache_time -1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf glance_store stores file,http
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf glance_store default_store file
[root@controller1 ~]# openstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/
[root@controller1 ~]# vim /etc/glance/glance-api.conf
...+
...+

[root@controller1 rsyslog.d]# >/etc/glance/glance-api.conf
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf DEFAULT debug True
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf DEFAULT verbose True
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf DEFAULT bind_host controller1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf DEFAULT bind_port 9191
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf DEFAULT worker 4
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_host 9.110.187.128
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_port 5672
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_userid openstack
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_password password
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_use_ssl false
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_ha_queues True
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_retry_interval 1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_retry_backoff 2
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit rabbit_max_retries 0
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf oslo_messaging_rabbit amqp_durable_queues false
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:password@demo.openstack.com/glance
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf database idle_timeout 3600
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf database max_pool_size 30
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf database max_retries -1
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://demo.openstack.com:5000
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://demo.openstack.com:35357
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller1:11211,controller2:11211,controller3:11211
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password password
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf glance_store filesystem_store_datadir /var/lib/glance/images/
[root@controller1 ~]# openstack-config --set /etc/glance/glance-registry.conf glance_store os_region_name RegionOne
[root@controller1 ~]# vim /etc/glance/glance-registry.conf
...+
...+


- install & config nova;
- install & config cinder;
- install & config neutron;
- install Dashboard;
- add Serv & Res to Pacemaker;
- Compute node deployment.

# Virt Env list
3 x Controller Nodes (8G, 300GB - )
    192.168.56.120 controller1 controller1.test.com                                 9.110.187.4
    192.168.56.121 controller2 controller2.test.com                                 9.110.187.5
    192.168.56.122 controller3 controller3.test.com                                 9.110.187.6
2 x Compute Nodes (16G, 300GB - )
    192.168.56.133 compute1 compute1.test.com
    192.168.56.134 compute2 compute2.test.com (glance-backend backend.test.com)
1 x Cinder Node (8G 400GB -)
    192.168.56.135 cinder1 cinder1.test.com

9.110.187.128 demo.openstack.com

# Network Config
public : 9.110.187.0/24
admin : 192.168.56.0/24
private：192.168.57.0/24
mapping network port：eno67109408

